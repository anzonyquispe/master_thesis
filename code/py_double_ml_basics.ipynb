{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python: Basics of Double Machine Learning\n",
    "\n",
    "**Remark**: This notebook has a long computation time due to the large number of simulations.\n",
    "\n",
    "This notebooks contains the detailed simulations according to the introduction to double machine learning in the [User Guide](https://docs.doubleml.org/stable/guide/basics.html) of the DoubleML package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import clone\n",
    "\n",
    "from doubleml import DoubleMLData\n",
    "from doubleml import DoubleMLPLR\n",
    "from doubleml.datasets import make_plr_CCDDHNR2018\n",
    "\n",
    "face_colors = sns.color_palette('pastel')\n",
    "edge_colors = sns.color_palette('dark')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generating Process (DGP)\n",
    "\n",
    "We consider the following partially linear model:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "y_i &= \\theta_0 d_i + g_0(x_i) + \\zeta_i, & \\zeta_i \\sim \\mathcal{N}(0,1), \\\\\n",
    "d_i &= m_0(x_i) + v_i, & v_i \\sim \\mathcal{N}(0,1),\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "with covariates $x_i \\sim \\mathcal{N}(0, \\Sigma)$, where $\\Sigma$ is a matrix with entries $\\Sigma_{kj} = 0.7^{|j-k|}$. We are interested in performing valid inference on the causal parameter $\\theta_0$. The true parameter $\\theta_0$ is set to $0.5$ in our simulation experiment.\n",
    "\n",
    "The nuisance functions are given by:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "m_0(x_i) &= x_{i,1} + \\frac{1}{4}  \\frac{\\exp(x_{i,3})}{1+\\exp(x_{i,3})}, \\\\\n",
    "g_0(x_i) &= \\frac{\\exp(x_{i,1})}{1+\\exp(x_{i,1})} + \\frac{1}{4} x_{i,3}.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate ``n_rep`` replications of the data generating process with sample size ``n_obs`` and compare the performance of different estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "n_rep = 1000\n",
    "n_obs = 500\n",
    "n_vars = 5\n",
    "alpha = 0.5\n",
    "\n",
    "data = list()\n",
    "\n",
    "# for i_rep in range(n_rep):\n",
    "#     (x, y, d) = make_plr_CCDDHNR2018(alpha=alpha, n_obs=n_obs, dim_x=n_vars, return_type='array')\n",
    "#     data.append((x, y, d))\n",
    "alldf = []\n",
    "for i_rep in range(n_rep):\n",
    "    (x, y, d) = make_did_SZ2020(n_obs=500, dgp_type=1, cross_sectional_data=False, return_type='array')\n",
    "    # Convert to DataFrame and add identifying columns\n",
    "    df = pd.DataFrame(x, columns=[f'x{i}' for i in range(x.shape[1])])\n",
    "    df['y'] = y\n",
    "    df['d'] = d\n",
    "    df['rep'] = i_rep\n",
    "    alldf.append(df)\n",
    "    data.append((x, y, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Bias in Simple ML-Approaches\n",
    "\n",
    "Naive inference that is based on a direct application of machine learning methods to estimate the causal parameter, $\\theta_0$, is generally invalid. The use of machine learning methods introduces a bias that arises due to regularization. A simple ML approach is given by randomly splitting the sample into two parts. On the auxiliary sample indexed by $i \\in I^C$ the nuisance function $g_0(X)$ is estimated with an ML method, for example a random forest learner. Given the estimate $\\hat{g}_0(X)$, the final estimate of $\\theta_0$ is obtained as ($n=N/2$) using the other half of observations indexed with $i \\in I$\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_0 = \\left(\\frac{1}{n} \\sum_{i\\in I} D_i^2\\right)^{-1} \\frac{1}{n} \\sum_{i\\in I} D_i (Y_i - \\hat{g}_0(X_i)).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this corresponds to a \"non-orthogonal\" score, which is not implemented in the DoubleML package, we need to define a custom callable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert to DataFrame\n",
    "# df = pd.concat(alldf)\n",
    "# df[['y', 'd', 'x0', 'x1', 'x2', 'x3']].to_csv('../data/simdata.csv', index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_orth_score(y, d, l_hat, m_hat, g_hat, smpls):\n",
    "    u_hat = y - g_hat\n",
    "    psi_a = -np.multiply(d, d)\n",
    "    psi_b = np.multiply(d, u_hat)\n",
    "    return psi_a, psi_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark that the estimator is not able to estimate $\\hat{g}_0(X)$ directly, but has to be based on a preliminary estimate of $\\hat{m}_0(X)$. All following estimators with ``score=\"IV-type\"`` are based on the same preliminary procedure. Furthermore, remark that we are using external predictions to avoid cross-fitting (for demonstration purposes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replication 25/1000\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m psi_b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmultiply(d[i_train] \u001b[38;5;241m-\u001b[39m ml_m\u001b[38;5;241m.\u001b[39mpredict(x[i_train, :]), y[i_train] \u001b[38;5;241m-\u001b[39m ml_l\u001b[38;5;241m.\u001b[39mpredict(x[i_train, :]))\n\u001b[1;32m     24\u001b[0m theta_initial \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mnanmean(psi_b) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmean(psi_a)\n\u001b[0;32m---> 25\u001b[0m ml_g\u001b[38;5;241m.\u001b[39mfit(x[i_train, :], y[i_train] \u001b[38;5;241m-\u001b[39m theta_initial \u001b[38;5;241m*\u001b[39m d[i_train])\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# create out-of-sample predictions\u001b[39;00m\n\u001b[1;32m     28\u001b[0m l_hat \u001b[38;5;241m=\u001b[39m ml_l\u001b[38;5;241m.\u001b[39mpredict(x[i_est, :])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightgbm/sklearn.py:1398\u001b[0m, in \u001b[0;36mLGBMRegressor.fit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   1382\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1383\u001b[0m     X: _LGBM_ScikitMatrixLike,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1395\u001b[0m     init_model: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Path, Booster, LGBMModel]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1396\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLGBMRegressor\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1398\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m   1399\u001b[0m         X,\n\u001b[1;32m   1400\u001b[0m         y,\n\u001b[1;32m   1401\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m   1402\u001b[0m         init_score\u001b[38;5;241m=\u001b[39minit_score,\n\u001b[1;32m   1403\u001b[0m         eval_set\u001b[38;5;241m=\u001b[39meval_set,\n\u001b[1;32m   1404\u001b[0m         eval_names\u001b[38;5;241m=\u001b[39meval_names,\n\u001b[1;32m   1405\u001b[0m         eval_sample_weight\u001b[38;5;241m=\u001b[39meval_sample_weight,\n\u001b[1;32m   1406\u001b[0m         eval_init_score\u001b[38;5;241m=\u001b[39meval_init_score,\n\u001b[1;32m   1407\u001b[0m         eval_metric\u001b[38;5;241m=\u001b[39meval_metric,\n\u001b[1;32m   1408\u001b[0m         feature_name\u001b[38;5;241m=\u001b[39mfeature_name,\n\u001b[1;32m   1409\u001b[0m         categorical_feature\u001b[38;5;241m=\u001b[39mcategorical_feature,\n\u001b[1;32m   1410\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m   1411\u001b[0m         init_model\u001b[38;5;241m=\u001b[39minit_model,\n\u001b[1;32m   1412\u001b[0m     )\n\u001b[1;32m   1413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightgbm/sklearn.py:1049\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1046\u001b[0m evals_result: _EvalResultDict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1047\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[0;32m-> 1049\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[1;32m   1050\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m   1051\u001b[0m     train_set\u001b[38;5;241m=\u001b[39mtrain_set,\n\u001b[1;32m   1052\u001b[0m     num_boost_round\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_estimators,\n\u001b[1;32m   1053\u001b[0m     valid_sets\u001b[38;5;241m=\u001b[39mvalid_sets,\n\u001b[1;32m   1054\u001b[0m     valid_names\u001b[38;5;241m=\u001b[39meval_names,\n\u001b[1;32m   1055\u001b[0m     feval\u001b[38;5;241m=\u001b[39meval_metrics_callable,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m     init_model\u001b[38;5;241m=\u001b[39minit_model,\n\u001b[1;32m   1057\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m   1058\u001b[0m )\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;66;03m# This populates the property self.n_features_, the number of features in the fitted model,\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# and so should only be set after fitting.\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;66;03m# The related property self._n_features_in, which populates self.n_features_in_,\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;66;03m# is set BEFORE fitting.\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\u001b[38;5;241m.\u001b[39mnum_feature()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightgbm/engine.py:322\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    311\u001b[0m     cb(\n\u001b[1;32m    312\u001b[0m         callback\u001b[38;5;241m.\u001b[39mCallbackEnv(\n\u001b[1;32m    313\u001b[0m             model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    319\u001b[0m         )\n\u001b[1;32m    320\u001b[0m     )\n\u001b[0;32m--> 322\u001b[0m booster\u001b[38;5;241m.\u001b[39mupdate(fobj\u001b[38;5;241m=\u001b[39mfobj)\n\u001b[1;32m    324\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/lightgbm/basic.py:4155\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   4152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   4153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4154\u001b[0m _safe_call(\n\u001b[0;32m-> 4155\u001b[0m     _LIB\u001b[38;5;241m.\u001b[39mLGBM_BoosterUpdateOneIter(\n\u001b[1;32m   4156\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle,\n\u001b[1;32m   4157\u001b[0m         ctypes\u001b[38;5;241m.\u001b[39mbyref(is_finished),\n\u001b[1;32m   4158\u001b[0m     )\n\u001b[1;32m   4159\u001b[0m )\n\u001b[1;32m   4160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[1;32m   4161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(1111)\n",
    "\n",
    "ml_l = LGBMRegressor(n_estimators=300, learning_rate=0.1, verbose=-1)\n",
    "ml_m = LGBMRegressor(n_estimators=300, learning_rate=0.1, verbose=-1)\n",
    "\n",
    "ml_g = clone(ml_l)\n",
    "\n",
    "theta_nonorth = np.full(n_rep, np.nan)\n",
    "se_nonorth = np.full(n_rep, np.nan) \n",
    "\n",
    "for i_rep in range(n_rep):\n",
    "    print(f'Replication {i_rep+1}/{n_rep}', end='\\r')\n",
    "    (x, y, d) = data[i_rep]\n",
    "    \n",
    "    # choose a random sample for training and estimation\n",
    "    i_train, i_est = train_test_split(np.arange(n_obs), test_size=0.5, random_state=42)\n",
    "    \n",
    "    # fit the ML algorithms on the training sample\n",
    "    ml_l.fit(x[i_train, :], y[i_train])\n",
    "    ml_m.fit(x[i_train, :], d[i_train])\n",
    "\n",
    "    psi_a = -np.multiply(d[i_train] - ml_m.predict(x[i_train, :]), d[i_train] - ml_m.predict(x[i_train, :]))\n",
    "    psi_b = np.multiply(d[i_train] - ml_m.predict(x[i_train, :]), y[i_train] - ml_l.predict(x[i_train, :]))\n",
    "    theta_initial = -np.nanmean(psi_b) / np.nanmean(psi_a)\n",
    "    ml_g.fit(x[i_train, :], y[i_train] - theta_initial * d[i_train])\n",
    "\n",
    "    # create out-of-sample predictions\n",
    "    l_hat = ml_l.predict(x[i_est, :])\n",
    "    m_hat = ml_m.predict(x[i_est, :])\n",
    "    g_hat = ml_g.predict(x[i_est, :])\n",
    "\n",
    "    external_predictions = {\n",
    "        'd': {\n",
    "            'ml_l': l_hat.reshape(-1, 1),\n",
    "            'ml_m': m_hat.reshape(-1, 1),\n",
    "            'ml_g': g_hat.reshape(-1, 1)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    obj_dml_data = DoubleMLData.from_arrays(x[i_est, :], y[i_est], d[i_est])\n",
    "    obj_dml_plr_nonorth = DoubleMLPLR(obj_dml_data,\n",
    "                                    ml_l, ml_m, ml_g,\n",
    "                                    n_folds=2,\n",
    "                                    score=non_orth_score)\n",
    "    obj_dml_plr_nonorth.fit(external_predictions=external_predictions)\n",
    "    theta_nonorth[i_rep] = obj_dml_plr_nonorth.coef[0]\n",
    "    se_nonorth[i_rep] = obj_dml_plr_nonorth.se[0]\n",
    "\n",
    "fig_non_orth, ax = plt.subplots(constrained_layout=True);\n",
    "ax = sns.histplot((theta_nonorth - alpha)/se_nonorth,\n",
    "                color=face_colors[0], edgecolor = edge_colors[0],\n",
    "                stat='density', bins=30, label='Non-orthogonal ML');\n",
    "ax.axvline(0., color='k');\n",
    "xx = np.arange(-5, +5, 0.001)\n",
    "yy = stats.norm.pdf(xx)\n",
    "ax.plot(xx, yy, color='k', label='$\\\\mathcal{N}(0, 1)$');\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0));\n",
    "ax.set_xlim([-6., 6.]);\n",
    "ax.set_xlabel('$(\\hat{\\\\theta}_0 - \\\\theta_0)/\\hat{\\sigma}$');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularization bias in the simple ML-approach is caused by the slow convergence of $\\hat{\\theta}_0$\n",
    "\n",
    "$$\n",
    "|\\sqrt{n} (\\hat{\\theta}_0 - \\theta_0) | \\rightarrow_{P} \\infty\n",
    "$$\n",
    "\n",
    "i.e., slower than $1/\\sqrt{n}$.\n",
    "The driving factor is the bias that arises by learning $g$ with a random forest or any other ML technique.\n",
    "A heuristic illustration is given by\n",
    "\n",
    "$$\n",
    "\\sqrt{n}(\\hat{\\theta}_0 - \\theta_0) = \\underbrace{\\left(\\frac{1}{n} \\sum_{i\\in I} D_i^2\\right)^{-1} \\frac{1}{n} \\sum_{i\\in I} D_i \\zeta_i}_{=:a}\n",
    "+  \\underbrace{\\left(\\frac{1}{n} \\sum_{i\\in I} D_i^2\\right)^{-1} \\frac{1}{n} \\sum_{i\\in I} D_i (g_0(X_i) - \\hat{g}_0(X_i))}_{=:b}.\n",
    "$$\n",
    "\n",
    "$a$ is approximately Gaussian under mild conditions.\n",
    "However, $b$ (the regularization bias) diverges in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overcoming regularization bias by orthogonalization\n",
    "\n",
    "To overcome the regularization bias we can partial out the effect of $X$ from $D$ to obtain the orthogonalized regressor $V = D - m(X)$. We then use the final estimate\n",
    "\n",
    "$$\n",
    "\\check{\\theta}_0 = \\left(\\frac{1}{n} \\sum_{i\\in I} \\hat{V}_i D_i\\right)^{-1} \\frac{1}{n} \\sum_{i\\in I} \\hat{V}_i (Y_i - \\hat{g}_0(X_i)).\n",
    "$$\n",
    "\n",
    "The following figure shows the distribution of the resulting estimates $\\hat{\\theta}_0$ without sample-splitting. Again, we are using external predictions to avoid cross-fitting (for demonstration purposes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2222)\n",
    "\n",
    "theta_orth_nosplit = np.full(n_rep, np.nan)\n",
    "se_orth_nosplit = np.full(n_rep, np.nan)\n",
    "\n",
    "for i_rep in range(n_rep):\n",
    "    print(f'Replication {i_rep+1}/{n_rep}', end='\\r')\n",
    "    (x, y, d) = data[i_rep]\n",
    "\n",
    "    # fit the ML algorithms on the training sample\n",
    "    ml_l.fit(x, y)\n",
    "    ml_m.fit(x, d)\n",
    "\n",
    "    psi_a = -np.multiply(d - ml_m.predict(x), d - ml_m.predict(x))\n",
    "    psi_b = np.multiply(d - ml_m.predict(x), y - ml_l.predict(x))\n",
    "    theta_initial = -np.nanmean(psi_b) / np.nanmean(psi_a)\n",
    "    ml_g.fit(x, y - theta_initial * d)\n",
    "\n",
    "    l_hat = ml_l.predict(x)\n",
    "    m_hat = ml_m.predict(x)\n",
    "    g_hat = ml_g.predict(x)\n",
    "\n",
    "    external_predictions = {\n",
    "        'd': {\n",
    "            'ml_l': l_hat.reshape(-1, 1),\n",
    "            'ml_m': m_hat.reshape(-1, 1),\n",
    "            'ml_g': g_hat.reshape(-1, 1)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    obj_dml_data = DoubleMLData.from_arrays(x, y, d)\n",
    "    \n",
    "    obj_dml_plr_orth_nosplit = DoubleMLPLR(obj_dml_data,\n",
    "                                        ml_l, ml_m, ml_g,\n",
    "                                        score='IV-type')\n",
    "    obj_dml_plr_orth_nosplit.fit(external_predictions=external_predictions)\n",
    "    theta_orth_nosplit[i_rep] = obj_dml_plr_orth_nosplit.coef[0]\n",
    "    se_orth_nosplit[i_rep] = obj_dml_plr_orth_nosplit.se[0]\n",
    "\n",
    "fig_orth_nosplit, ax = plt.subplots(constrained_layout=True);\n",
    "ax = sns.histplot((theta_orth_nosplit - alpha)/se_orth_nosplit,\n",
    "                color=face_colors[1], edgecolor = edge_colors[1],\n",
    "                stat='density', bins=30, label='Double ML (no sample splitting)');\n",
    "ax.axvline(0., color='k');\n",
    "xx = np.arange(-5, +5, 0.001)\n",
    "yy = stats.norm.pdf(xx)\n",
    "ax.plot(xx, yy, color='k', label='$\\\\mathcal{N}(0, 1)$');\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0));    \n",
    "ax.set_xlim([-6., 6.]);\n",
    "ax.set_xlabel('$(\\hat{\\\\theta}_0 - \\\\theta_0)/\\hat{\\sigma}$');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the nuisance models $\\hat{g}_0()$ and $\\hat{m}()$ are estimated on the whole dataset, which is also used for obtaining the final estimate $\\check{\\theta}_0$, another bias is observed.\n",
    "\n",
    "## Sample splitting to remove bias induced by overfitting\n",
    "\n",
    "Using sample splitting, i.e., estimate the nuisance models $\\hat{g}_0()$ and $\\hat{m}()$ on one part of the data (training data) and estimate $\\check{\\theta}_0$ on the other part of the data (test data), overcomes the bias induced by overfitting. We can exploit the benefits of cross-fitting by switching the role of the training and test sample. Cross-fitting performs well empirically because the entire sample can be used for estimation.\n",
    "\n",
    "The following figure shows the distribution of the resulting estimates $\\hat{\\theta}_0$ with orthogonal score and sample-splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3333)\n",
    "\n",
    "theta_dml = np.full(n_rep, np.nan)\n",
    "se_dml = np.full(n_rep, np.nan)\n",
    "\n",
    "for i_rep in range(n_rep):\n",
    "    print(f'Replication {i_rep+1}/{n_rep}', end='\\r')\n",
    "    (x, y, d) = data[i_rep]\n",
    "    obj_dml_data = DoubleMLData.from_arrays(x, y, d)\n",
    "    obj_dml_plr = DoubleMLPLR(obj_dml_data,\n",
    "                            ml_l, ml_m, ml_g,\n",
    "                            n_folds=2,\n",
    "                            score='IV-type')\n",
    "    obj_dml_plr.fit()\n",
    "    theta_dml[i_rep] = obj_dml_plr.coef[0]\n",
    "    se_dml[i_rep] = obj_dml_plr.se[0]\n",
    "\n",
    "fig_dml, ax = plt.subplots(constrained_layout=True);\n",
    "ax = sns.histplot((theta_dml - alpha)/se_dml,\n",
    "                color=face_colors[2], edgecolor = edge_colors[2],\n",
    "                stat='density', bins=30, label='Double ML with cross-fitting');\n",
    "ax.axvline(0., color='k');\n",
    "xx = np.arange(-5, +5, 0.001)\n",
    "yy = stats.norm.pdf(xx)\n",
    "ax.plot(xx, yy, color='k', label='$\\\\mathcal{N}(0, 1)$');\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0));\n",
    "ax.set_xlim([-6., 6.]);\n",
    "ax.set_xlabel('$(\\hat{\\\\theta}_0 - \\\\theta_0)/\\hat{\\sigma}$');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double/debiased machine learning\n",
    "\n",
    "To illustrate the benefits of the auxiliary prediction step in the DML framework we write the error as\n",
    "\n",
    "$$\n",
    "\\sqrt{n}(\\check{\\theta}_0 - \\theta_0) = a^* + b^* + c^*\n",
    "$$\n",
    "\n",
    "Chernozhukov et al. (2018) argues that:\n",
    "\n",
    "The first term\n",
    "\n",
    "$$\n",
    "a^* := (EV^2)^{-1} \\frac{1}{\\sqrt{n}} \\sum_{i\\in I} V_i \\zeta_i\n",
    "$$\n",
    "\n",
    "will be asymptotically normally distributed.\n",
    "\n",
    "The second term\n",
    "\n",
    "$$\n",
    "b^* := (EV^2)^{-1} \\frac{1}{\\sqrt{n}} \\sum_{i\\in I} (\\hat{m}(X_i) - m(X_i)) (\\hat{g}_0(X_i) - g_0(X_i))\n",
    "$$\n",
    "\n",
    "vanishes asymptotically for many data generating processes.\n",
    "\n",
    "The third term $c^*$ vanishes in probability if sample splitting is applied. Finally, let us compare all distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbsphinx-thumbnail"
    ]
   },
   "outputs": [],
   "source": [
    "fig_all, ax = plt.subplots(constrained_layout=True);\n",
    "ax = sns.histplot((theta_nonorth - alpha)/se_nonorth,\n",
    "                 color=face_colors[0], edgecolor = edge_colors[0],\n",
    "                 stat='density', bins=30, label='Non-orthogonal ML');\n",
    "sns.histplot((theta_orth_nosplit - alpha)/se_orth_nosplit,\n",
    "             color=face_colors[1], edgecolor = edge_colors[1],\n",
    "             stat='density', bins=30, label='Double ML (no sample splitting)');\n",
    "sns.histplot((theta_dml - alpha)/se_dml,\n",
    "             color=face_colors[2], edgecolor = edge_colors[2],\n",
    "             stat='density', bins=30, label='Double ML with cross-fitting');\n",
    "ax.axvline(0., color='k');\n",
    "xx = np.arange(-5, +5, 0.001)\n",
    "yy = stats.norm.pdf(xx)\n",
    "ax.plot(xx, yy, color='k', label='$\\\\mathcal{N}(0, 1)$');\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0));\n",
    "ax.set_xlim([-6., 6.]);\n",
    "ax.set_xlabel('$(\\hat{\\\\theta}_0 - \\\\theta_0)/\\hat{\\sigma}$');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partialling out score\n",
    "\n",
    "Another debiased estimator, based on the partialling-out approach of Robinson(1988), is\n",
    "\n",
    "$$\n",
    "\\check{\\theta}_0 = \\left(\\frac{1}{n} \\sum_{i\\in I} \\hat{V}_i \\hat{V}_i \\right)^{-1} \\frac{1}{n} \\sum_{i\\in I} \\hat{V}_i (Y_i - \\hat{\\ell}_0(X_i)),\n",
    "$$\n",
    "\n",
    "with $\\ell_0(X_i) = E(Y|X)$.\n",
    "All nuisance parameters for the estimator with `score='partialling out'` are conditional mean functions, which can be directly estimated using ML methods. This is a minor advantage over the estimator with `score='IV-type'`.\n",
    "In the following, we repeat the above analysis with `score='partialling out'`. In a first part of the analysis, we estimate $\\theta_0$ without sample splitting. Again we observe a bias from overfitting.\n",
    "\n",
    "The following figure shows the distribution of the resulting estimates $\\hat{\\theta}_0$ without sample-splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4444)\n",
    "\n",
    "theta_orth_po_nosplit = np.full(n_rep, np.nan)\n",
    "se_orth_po_nosplit = np.full(n_rep, np.nan)\n",
    "\n",
    "for i_rep in range(n_rep):\n",
    "     print(f'Replication {i_rep+1}/{n_rep}', end='\\r')\n",
    "     (x, y, d) = data[i_rep]\n",
    "\n",
    "     # fit the ML algorithms on the training sample\n",
    "     ml_l.fit(x, y)\n",
    "     ml_m.fit(x, d)\n",
    "\n",
    "     l_hat = ml_l.predict(x)\n",
    "     m_hat = ml_m.predict(x)\n",
    "\n",
    "     external_predictions = {\n",
    "          'd': {\n",
    "               'ml_l': l_hat.reshape(-1, 1),\n",
    "               'ml_m': m_hat.reshape(-1, 1),\n",
    "          }\n",
    "     }\n",
    "\n",
    "     obj_dml_plr_orth_nosplit = DoubleMLPLR(obj_dml_data,\n",
    "                                         ml_l, ml_m,\n",
    "                                         score='partialling out')\n",
    "     obj_dml_plr_orth_nosplit.fit(external_predictions=external_predictions)\n",
    "     theta_orth_po_nosplit[i_rep] = obj_dml_plr_orth_nosplit.coef[0]\n",
    "     se_orth_po_nosplit[i_rep] = obj_dml_plr_orth_nosplit.se[0]\n",
    "\n",
    "fig_po_nosplit, ax = plt.subplots(constrained_layout=True);\n",
    "ax = sns.histplot((theta_orth_po_nosplit - alpha)/se_orth_po_nosplit,\n",
    "                 color=face_colors[1], edgecolor = edge_colors[1],\n",
    "                 stat='density', bins=30, label='Double ML (no sample splitting)');\n",
    "ax.axvline(0., color='k');\n",
    "xx = np.arange(-5, +5, 0.001)\n",
    "yy = stats.norm.pdf(xx)\n",
    "ax.plot(xx, yy, color='k', label='$\\\\mathcal{N}(0, 1)$');\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0));\n",
    "ax.set_xlim([-6., 6.]);\n",
    "ax.set_xlabel('$(\\hat{\\\\theta}_0 - \\\\theta_0)/\\hat{\\sigma}$');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using sample splitting, overcomes the bias induced by overfitting.\n",
    "Again, the implementation automatically applies cross-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5555)\n",
    "\n",
    "theta_dml_po = np.full(n_rep, np.nan)\n",
    "se_dml_po = np.full(n_rep, np.nan)\n",
    "\n",
    "for i_rep in range(n_rep):\n",
    "     print(f'Replication {i_rep+1}/{n_rep}', end='\\r')\n",
    "     (x, y, d) = data[i_rep]\n",
    "     obj_dml_data = DoubleMLData.from_arrays(x, y, d)\n",
    "     obj_dml_plr = DoubleMLPLR(obj_dml_data,\n",
    "                             ml_l, ml_m,\n",
    "                             n_folds=2,\n",
    "                             score='partialling out')\n",
    "     obj_dml_plr.fit()\n",
    "     theta_dml_po[i_rep] = obj_dml_plr.coef[0]\n",
    "     se_dml_po[i_rep] = obj_dml_plr.se[0]\n",
    " \n",
    "fig_po_dml, ax = plt.subplots(constrained_layout=True);\n",
    "ax = sns.histplot((theta_dml_po - alpha)/se_dml_po,\n",
    "                 color=face_colors[2], edgecolor = edge_colors[2],\n",
    "                 stat='density', bins=30, label='Double ML with cross-fitting');\n",
    "ax.axvline(0., color='k');\n",
    "xx = np.arange(-5, +5, 0.001)\n",
    "yy = stats.norm.pdf(xx)\n",
    "ax.plot(xx, yy, color='k', label='$\\\\mathcal{N}(0, 1)$');\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0));\n",
    "ax.set_xlim([-6., 6.]);\n",
    "ax.set_xlabel('$(\\hat{\\\\theta}_0 - \\\\theta_0)/\\hat{\\sigma}$');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us compare all distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_po_all, ax = plt.subplots(constrained_layout=True);\n",
    "ax = sns.histplot((theta_nonorth - alpha)/se_nonorth,\n",
    "                color=face_colors[0], edgecolor = edge_colors[0],\n",
    "                stat='density', bins=30, label='Non-orthogonal ML');\n",
    "sns.histplot((theta_orth_po_nosplit - alpha)/se_orth_po_nosplit,\n",
    "            color=face_colors[1], edgecolor = edge_colors[1],\n",
    "            stat='density', bins=30, label='Double ML (no sample splitting)');\n",
    "sns.histplot((theta_dml_po - alpha)/se_dml_po,\n",
    "            color=face_colors[2], edgecolor = edge_colors[2],\n",
    "            stat='density', bins=30, label='Double ML with cross-fitting');\n",
    "ax.axvline(0., color='k');\n",
    "xx = np.arange(-5, +5, 0.001)\n",
    "yy = stats.norm.pdf(xx)\n",
    "ax.plot(xx, yy, color='k', label='$\\\\mathcal{N}(0, 1)$');\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0));\n",
    "ax.set_xlim([-6., 6.]);\n",
    "ax.set_xlabel('$(\\hat{\\\\theta}_0 - \\\\theta_0)/\\hat{\\sigma}$');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all figures\n",
    "fig_non_orth.savefig('../guide/figures/py_non_orthogonal.svg', dpi=300, bbox_inches='tight', format='svg')\n",
    "fig_orth_nosplit.savefig('../guide/figures/py_dml_nosplit.svg', dpi=300, bbox_inches='tight', format='svg')\n",
    "fig_dml.savefig('../guide/figures/py_dml.svg', dpi=300, bbox_inches='tight', format='svg')\n",
    "fig_all.savefig('../guide/figures/py_all.svg', dpi=300, bbox_inches='tight', format='svg')\n",
    "\n",
    "fig_po_nosplit.savefig('../guide/figures/py_dml_po_nosplit.svg', dpi=300, bbox_inches='tight', format='svg')\n",
    "fig_po_dml.savefig('../guide/figures/py_dml_po.svg', dpi=300, bbox_inches='tight', format='svg')\n",
    "fig_po_all.savefig('../guide/figures/py_po_all.svg', dpi=300, bbox_inches='tight', format='svg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
