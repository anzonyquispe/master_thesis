{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7d679434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from patsy import dmatrix, EvalEnvironment\n",
    "\n",
    "# Define a simple raw polynomial function.\n",
    "def poly(x, degree, raw=True):\n",
    "    # Return columns: x, x^2, ..., x^degree\n",
    "    # x should be a numpy array.\n",
    "    x = np.asarray(x)\n",
    "    return np.column_stack([x**i for i in range(1, degree+1)])\n",
    "\n",
    "# Create an evaluation environment that includes our poly function.\n",
    "env = EvalEnvironment({'poly': poly})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "42b311f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import scale, StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from tensorflow import keras\n",
    "from patsy import dmatrix\n",
    "from sklearn.model_selection import KFold\n",
    "# For quantile regression (placeholder use)\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.quantile_regression import QuantReg\n",
    "\n",
    "# Global parameters\n",
    "c_param = 0.5\n",
    "alpha_param = 0.1\n",
    "tol = 1e-6\n",
    "l = 0.1  # dictionary weight for intercept\n",
    "\n",
    "##########################\n",
    "# Basic Helper Functions #\n",
    "##########################\n",
    "\n",
    "def two_norm(x):\n",
    "    return np.linalg.norm(x)\n",
    "\n",
    "def b(d, z):\n",
    "    # returns vector: [1, d, (elements of z)]\n",
    "    z = np.array(z).flatten()\n",
    "    return np.concatenate(([1], [d], z))\n",
    "\n",
    "def b2(d, z):\n",
    "    # returns vector: [1, d, (elements of z), d*z (elementwise)]\n",
    "    z = np.array(z).flatten()\n",
    "    return np.concatenate(([1], [d], z, d * z))\n",
    "\n",
    "def b_PD(d, z):\n",
    "    # identity for PD elasticity (here simply returns d)\n",
    "    return np.array([d])\n",
    "\n",
    "def m_ATE(y, d, z, gamma_func):\n",
    "    # returns gamma(1,z) - gamma(0,z)\n",
    "    return gamma_func(1, z) - gamma_func(0, z)\n",
    "\n",
    "def m_PD(y, x, x_up, x_down, delta, gamma_func):\n",
    "    # elasticity moment function\n",
    "    return (gamma_func(x_up, 0) - gamma_func(x_down, 0)) / delta\n",
    "\n",
    "def m2(y, d, z, gamma_func):\n",
    "    return y * gamma_func(d, z)\n",
    "\n",
    "def get_p(args, est_type, X, b_func):\n",
    "    if est_type == \"ATE\":\n",
    "        return len(b_func(args['T'][0], X[0, :]))\n",
    "    else:\n",
    "        return X.shape[1]\n",
    "\n",
    "def get_n(args, est_type, X):\n",
    "    if est_type == \"ATE\":\n",
    "        return len(args['T'])\n",
    "    else:\n",
    "        return X.shape[0]\n",
    "\n",
    "def psi_tilde(args, est_type, y, z, alpha, gamma_func):\n",
    "    if est_type == \"ATE\":\n",
    "        # here args is a dict with a scalar 'T'\n",
    "        d = args['T']\n",
    "        return m_ATE(y, d, z, gamma_func) + alpha(d, z) * (y - gamma_func(d, z))\n",
    "    else:\n",
    "        x_up = args['X.up']\n",
    "        x_down = args['X.down']\n",
    "        delta = args['delta']\n",
    "        return m_PD(y, z, x_up, x_down, delta, gamma_func) + alpha(z, 0) * (y - gamma_func(z, 0))\n",
    "\n",
    "def psi_tilde_bias(args, est_type, y, z, alpha, gamma_func):\n",
    "    if est_type == \"ATE\":\n",
    "        d = args['d']\n",
    "        return m_ATE(y, d, z, gamma_func)\n",
    "    else:\n",
    "        return m_PD(y, z, args['X.up'], args['X.down'], args['delta'], gamma_func)\n",
    "\n",
    "def get_MNG(args, est_type, Y, X, b_func):\n",
    "    if est_type == \"ATE\":\n",
    "        T = args['T']\n",
    "        n_nl = len(T)\n",
    "        p = len(b_func(T[0], X[0, :]))\n",
    "        B = np.zeros((n_nl, p))\n",
    "        M = np.zeros((p, n_nl))\n",
    "        N = np.zeros((p, n_nl))\n",
    "        for i in range(n_nl):\n",
    "            B[i, :] = b_func(T[i], X[i, :])\n",
    "            # here we use the dictionary function in place of gamma\n",
    "            M[:, i] = m_ATE(Y[i], T[i], X[i, :], b_func)\n",
    "            N[:, i] = m2(Y[i], T[i], X[i, :], b_func)\n",
    "        M_hat = np.mean(M, axis=1)\n",
    "        N_hat = np.mean(N, axis=1)\n",
    "        G_hat = (B.T @ B) / n_nl\n",
    "        return M_hat, N_hat, G_hat, B\n",
    "    else:\n",
    "        X_up = args['X.up']\n",
    "        X_down = args['X.down']\n",
    "        delta = args['delta']\n",
    "        n = X.shape[0]\n",
    "        p = X.shape[1]\n",
    "        M = np.zeros((p, n))\n",
    "        N = np.zeros((p, n))\n",
    "        for i in range(n):\n",
    "            M[:, i] = (X_up[i, :] - X_down[i, :]) / delta\n",
    "            N[:, i] = Y[i] * X[i, :]\n",
    "        M_hat = np.mean(M, axis=1)\n",
    "        N_hat = np.mean(N, axis=1)\n",
    "        G_hat = (X.T @ X) / n\n",
    "        return M_hat, N_hat, G_hat, X\n",
    "\n",
    "#############################\n",
    "# Learning Algorithm Blocks #\n",
    "#############################\n",
    "\n",
    "# RMD_dantzig: placeholder using quantile regression approach.\n",
    "def RMD_dantzig(M, G, D, lambda_val=0, sparse=True):\n",
    "    p = G.shape[1]\n",
    "    zp = np.zeros(p)\n",
    "    L_vec = np.concatenate(([l], np.ones(p-1)))\n",
    "    A = np.linalg.solve(np.diag(D), G)\n",
    "    R_mat = np.vstack([A, -A])\n",
    "    a = np.linalg.solve(np.diag(D), M)\n",
    "    r_vec = np.concatenate([a - lambda_val * L_vec, -a - lambda_val * L_vec])\n",
    "    # Placeholder: in R the code uses rq.fit.sfn; here we simply return a dummy least–squares solution.\n",
    "    beta_dummy = np.linalg.lstsq(G, M, rcond=None)[0].flatten()\n",
    "    return {'coefficients': beta_dummy}\n",
    "\n",
    "def RMD_lasso(M, G, D, lambda_val=0, control=None, beta_start=None):\n",
    "    if control is None:\n",
    "        control = {'maxIter': 1000, 'optTol': 1e-5, 'zeroThreshold': 1e-6}\n",
    "    p = G.shape[1]\n",
    "    L_vec = np.concatenate(([l], np.ones(p-1)))\n",
    "    lambda_vec = lambda_val * L_vec * D\n",
    "    if beta_start is None:\n",
    "        beta = np.zeros(p)\n",
    "    else:\n",
    "        beta = beta_start.copy()\n",
    "    wp = [beta.copy()]\n",
    "    mm = 1\n",
    "    while mm < control['maxIter']:\n",
    "        beta_old = beta.copy()\n",
    "        for j in range(p):\n",
    "            rho = M[j] - np.dot(G[j, :], beta) + G[j, j] * beta[j]\n",
    "            z_val = G[j, j]\n",
    "            if np.isnan(rho):\n",
    "                beta[j] = 0\n",
    "                continue\n",
    "            if rho < -lambda_vec[j]:\n",
    "                beta[j] = (rho + lambda_vec[j]) / z_val\n",
    "            elif abs(rho) <= lambda_vec[j]:\n",
    "                beta[j] = 0\n",
    "            elif rho > lambda_vec[j]:\n",
    "                beta[j] = (rho - lambda_vec[j]) / z_val\n",
    "        wp.append(beta.copy())\n",
    "        if np.sum(np.abs(beta - beta_old)) < control['optTol']:\n",
    "            break\n",
    "        mm += 1\n",
    "    beta[np.abs(beta) < control['zeroThreshold']] = 0\n",
    "    return {'coefficients': beta, 'coef_list': wp, 'num_it': mm}\n",
    "\n",
    "def get_D(args, est_type, Y, X, m_func, rho_hat, b_func):\n",
    "    n = get_n(args, est_type, X)\n",
    "    p = get_p(args, est_type, X, b_func)\n",
    "    df_mat = np.zeros((p, n))\n",
    "    if est_type == \"ATE\":\n",
    "        T = args['T']\n",
    "        for i in range(n):\n",
    "            b_val = b_func(T[i], X[i, :])\n",
    "            dot_val = np.dot(rho_hat, b_val)\n",
    "            m_val = m_ATE(Y[i], T[i], X[i, :], b_func)\n",
    "            df_mat[:, i] = b_val * dot_val - m_val\n",
    "    else:\n",
    "        X_up = args['X.up']\n",
    "        X_down = args['X.down']\n",
    "        delta = args['delta']\n",
    "        for i in range(n):\n",
    "            dot_val = np.dot(rho_hat, X[i, :])\n",
    "            m_val = m_PD(Y[i], X[i, :], X_up[i, :], X_down[i, :], delta, b_func)\n",
    "            df_mat[:, i] = X[i, :] * dot_val - m_val\n",
    "    df_mat = df_mat ** 2\n",
    "    D2 = np.mean(df_mat, axis=1)\n",
    "    return np.sqrt(D2)\n",
    "\n",
    "def RMD_stable(args, est_type, Y, X, p0, D_LB, D_add, max_iter, b_func, is_alpha=True, is_lasso=True):\n",
    "    p = get_p(args, est_type, X, b_func)\n",
    "    n = get_n(args, est_type, X)\n",
    "    # Low-dimensional moments: use the first p0 columns of X.\n",
    "    X0 = X[:, :p0]\n",
    "    if est_type == \"ATE\":\n",
    "        args0 = {'T': args['T']}\n",
    "    else:\n",
    "        args0 = {'X.up': args['X.up'][:, :p0], 'X.down': args['X.down'][:, :p0], 'delta': args['delta']}\n",
    "    M_hat0, N_hat0, G_hat0, _ = get_MNG(args0, est_type, Y, X0, b_func)\n",
    "    rho_hat0 = np.linalg.solve(G_hat0, M_hat0)\n",
    "    if len(rho_hat0) < p:\n",
    "        rho_hat = np.concatenate([rho_hat0, np.zeros(p - len(rho_hat0))])\n",
    "    else:\n",
    "        rho_hat = rho_hat0\n",
    "    beta_hat0 = np.linalg.solve(G_hat0, N_hat0)\n",
    "    if len(beta_hat0) < p:\n",
    "        beta_hat = np.concatenate([beta_hat0, np.zeros(p - len(beta_hat0))])\n",
    "    else:\n",
    "        beta_hat = beta_hat0\n",
    "\n",
    "    M_hat, N_hat, G_hat, _ = get_MNG(args, est_type, Y, X, b_func)\n",
    "    lambda_val = c_param * norm.ppf(1 - alpha_param / (2 * p)) / np.sqrt(n)\n",
    "    k = 1\n",
    "    if is_alpha:\n",
    "        diff_rho = 1\n",
    "        while diff_rho > tol and k <= max_iter:\n",
    "            rho_hat_old = rho_hat.copy()\n",
    "            D_hat_rho = get_D(args, est_type, Y, X, m_func=m_ATE, rho_hat=rho_hat_old, b_func=b_func)\n",
    "            D_hat_rho = np.maximum(D_LB, D_hat_rho) + D_add\n",
    "            if is_lasso:\n",
    "                rho_hat = RMD_lasso(M_hat, G_hat, D_hat_rho, lambda_val)['coefficients']\n",
    "            else:\n",
    "                rho_hat = RMD_dantzig(M_hat, G_hat, D_hat_rho, lambda_val)['coefficients']\n",
    "            diff_rho = two_norm(rho_hat - rho_hat_old)\n",
    "            k += 1\n",
    "        print(\"k:\", k)\n",
    "        return rho_hat\n",
    "    else:\n",
    "        diff_beta = 1\n",
    "        while diff_beta > tol and k <= max_iter:\n",
    "            beta_hat_old = beta_hat.copy()\n",
    "            D_hat_beta = get_D(args, est_type, Y, X, m_func=m2, rho_hat=beta_hat_old, b_func=b_func)\n",
    "            D_hat_beta = np.maximum(D_LB, D_hat_beta) + D_add\n",
    "            if is_lasso:\n",
    "                beta_hat = RMD_lasso(N_hat, G_hat, D_hat_beta, lambda_val)['coefficients']\n",
    "            else:\n",
    "                beta_hat = RMD_dantzig(N_hat, G_hat, D_hat_beta, lambda_val)['coefficients']\n",
    "            diff_beta = two_norm(beta_hat - beta_hat_old)\n",
    "            k += 1\n",
    "        print(\"k:\", k)\n",
    "        return beta_hat\n",
    "\n",
    "###########################\n",
    "# Stage 1 & Stage 2 Steps #\n",
    "###########################\n",
    "\n",
    "arg_Forest = {'clas_nodesize': 1, 'reg_nodesize': 5, 'ntree': 1000, 'na_action': 'omit', 'replace': True}\n",
    "arg_Nnet = {'size': 8, 'maxit': 1000, 'decay': 0.01, 'MaxNWts': 10000, 'trace': False}\n",
    "\n",
    "def get_stage1(args, est_type, Y, X, p0, D_LB, D_add, max_iter, b_func, alpha_estimator=1, gamma_estimator=1):\n",
    "    p = get_p(args, est_type, X, b_func)\n",
    "    n = get_n(args, est_type, X)\n",
    "    MNG = get_MNG(args, est_type, Y, X, b_func)\n",
    "    B = MNG[3]\n",
    "    # alpha estimator\n",
    "    if alpha_estimator == 0:\n",
    "        rho_hat = RMD_stable(args, est_type, Y, X, p0, D_LB, D_add, max_iter, b_func, is_alpha=True, is_lasso=False)\n",
    "        def alpha_hat(d, z):\n",
    "            return np.dot(b_func(d, z), rho_hat)\n",
    "    elif alpha_estimator == 1:\n",
    "        rho_hat = RMD_stable(args, est_type, Y, X, p0, D_LB, D_add, max_iter, b_func, is_alpha=True, is_lasso=True)\n",
    "        def alpha_hat(d, z):\n",
    "            return np.dot(b_func(d, z), rho_hat)\n",
    "    # gamma estimator\n",
    "    if gamma_estimator == 0:\n",
    "        beta_hat = RMD_stable(args, est_type, Y, X, p0, D_LB, D_add, max_iter, b_func, is_alpha=False, is_lasso=False)\n",
    "        def gamma_hat(d, z):\n",
    "            return np.dot(b_func(d, z), beta_hat)\n",
    "    elif gamma_estimator == 1:\n",
    "        beta_hat = RMD_stable(args, est_type, Y, X, p0, D_LB, D_add, max_iter, b_func, is_alpha=False, is_lasso=True)\n",
    "        def gamma_hat(d, z):\n",
    "            return np.dot(b_func(d, z), beta_hat)\n",
    "    elif gamma_estimator == 2:\n",
    "        # Random Forest approach\n",
    "        rf = RandomForestRegressor(n_estimators=arg_Forest['ntree'],\n",
    "                                   min_samples_leaf=arg_Forest['reg_nodesize'],\n",
    "                                   bootstrap=arg_Forest['replace'], random_state=0)\n",
    "        rf.fit(B, Y)\n",
    "        def gamma_hat(d, z):\n",
    "            return rf.predict(b_func(d, z).reshape(1, -1))[0]\n",
    "    elif gamma_estimator == 3:\n",
    "        # Neural Net via MLPRegressor\n",
    "        scaler_B = StandardScaler().fit(B)\n",
    "        NN_B = scaler_B.transform(B)\n",
    "        scaler_Y = StandardScaler().fit(Y.reshape(-1, 1))\n",
    "        NN_Y = scaler_Y.transform(Y.reshape(-1, 1)).flatten()\n",
    "        nn = MLPRegressor(hidden_layer_sizes=(arg_Nnet['size'],),\n",
    "                          max_iter=arg_Nnet['maxit'],\n",
    "                          alpha=arg_Nnet['decay'],\n",
    "                          random_state=0)\n",
    "        nn.fit(NN_B, NN_Y)\n",
    "        def gamma_hat(d, z):\n",
    "            test = b2(d, z).reshape(1, -1)\n",
    "            test_scaled = scaler_B.transform(test)\n",
    "            NN_Y_hat = nn.predict(test_scaled)\n",
    "            Y_hat = NN_Y_hat * scaler_Y.scale_[0] + scaler_Y.mean_[0]\n",
    "            return Y_hat[0]\n",
    "    elif gamma_estimator == 4:\n",
    "        # 2-layer NN using Keras\n",
    "        scaler_B = StandardScaler().fit(B)\n",
    "        NN_B = scaler_B.transform(B)\n",
    "        scaler_Y = StandardScaler().fit(Y.reshape(-1, 1))\n",
    "        NN_Y = scaler_Y.transform(Y.reshape(-1, 1)).flatten()\n",
    "        model = keras.models.Sequential([\n",
    "            keras.layers.Dense(8, activation=\"relu\", input_shape=(NN_B.shape[1],)),\n",
    "            keras.layers.Dense(8, activation=\"relu\"),\n",
    "            keras.layers.Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "        model.fit(NN_B, NN_Y, epochs=100, batch_size=1, verbose=0)\n",
    "        def gamma_hat(d, z):\n",
    "            test = b(d, z).reshape(1, -1)\n",
    "            test_scaled = scaler_B.transform(test)\n",
    "            NN_Y_hat = model.predict(test_scaled, verbose=0)\n",
    "            Y_hat = NN_Y_hat * scaler_Y.scale_[0] + scaler_Y.mean_[0]\n",
    "            return Y_hat[0][0]\n",
    "    return alpha_hat, gamma_hat\n",
    "\n",
    "def rrr(args, est_type, Y, X, p0, D_LB, D_add, max_iter, dict_func, alpha_estimator, gamma_estimator, bias):\n",
    "    if est_type == 'ATE':\n",
    "        T = args['T']\n",
    "    else:\n",
    "        # For PD\n",
    "        X_up = args['X.up']\n",
    "        X_down = args['X.down']\n",
    "        delta = args['delta']\n",
    "    n = X.shape[0]\n",
    "    L_folds = 5\n",
    "    kf = KFold(n_splits=L_folds, shuffle=True, random_state=0)\n",
    "    Psi_tilde = []\n",
    "    fold_index = 0\n",
    "    for i in np.arange(0, 4):\n",
    "        \n",
    "        index = folds_info.iloc[:, i].dropna().astype(int).tolist()\n",
    "        total_indices = np.arange( 0, 9848 )\n",
    "        test_index = np.delete( total_indices, index, axis = 0 )\n",
    "        train_index = index\n",
    "\n",
    "        fold_index += 1\n",
    "        Y_l = Y[test_index]\n",
    "        Y_nl = Y[train_index]\n",
    "        X_l = X[test_index, :]\n",
    "        X_nl = X[train_index, :]\n",
    "        if est_type == \"ATE\":\n",
    "            T_arr = np.array(args['T'])\n",
    "            T_l = T_arr[test_index]\n",
    "            T_nl = T_arr[train_index]\n",
    "            n_l = len(T_l)\n",
    "            stage1_args = {'T': T_nl}\n",
    "        else:\n",
    "            X_up_full = args['X.up']\n",
    "            X_down_full = args['X.down']\n",
    "            X_up_l = X_up_full[test_index, :]\n",
    "            X_up_nl = X_up_full[train_index, :]\n",
    "            X_down_l = X_down_full[test_index, :]\n",
    "            n_l = X_l.shape[0]\n",
    "            stage1_args = {'X.up': X_up_nl, 'X.down': args['X.down'][train_index, :], 'delta': args['delta']}\n",
    "        alpha_hat, gamma_hat = get_stage1(stage1_args, est_type, Y_nl, X_nl, p0, D_LB, D_add, max_iter,\n",
    "                                          dict_func, alpha_estimator, gamma_estimator)\n",
    "        print(\"fold:\", fold_index)\n",
    "        Psi_tilde_l = []\n",
    "        for i in range(n_l):\n",
    "            if est_type == \"ATE\":\n",
    "                psi_args = {'T': T_l[i]}\n",
    "            else:\n",
    "                psi_args = {'X.up': X_up_l[i, :], 'X.down': X_down_l[i, :], 'delta': args['delta']}\n",
    "            if bias:\n",
    "                psi_val = psi_tilde_bias(psi_args, est_type, Y_l[i], X_l[i, :], None, gamma_hat)\n",
    "            else:\n",
    "                psi_val = psi_tilde(psi_args, est_type, Y_l[i], X_l[i, :], alpha_hat, gamma_hat)\n",
    "            Psi_tilde_l.append(psi_val)\n",
    "        Psi_tilde.extend(Psi_tilde_l)\n",
    "    Psi_tilde = np.array(Psi_tilde)\n",
    "    ate = np.mean(Psi_tilde)\n",
    "    Psi_infl = Psi_tilde - ate\n",
    "    var = np.mean(Psi_infl ** 2)\n",
    "    se = np.sqrt(var / n)\n",
    "    if est_type == \"ATE\":\n",
    "        T_arr = np.array(args['T'])\n",
    "        treated = np.sum(T_arr == 1)\n",
    "        untreated = np.sum(T_arr == 0)\n",
    "        out = (treated, untreated, ate, se)\n",
    "    else:\n",
    "        out = (n, ate, se)\n",
    "    return out\n",
    "\n",
    "def printer(spec1, est_type):\n",
    "    if est_type == \"ATE\":\n",
    "        print(f\" treated: {spec1[0]} untreated: {spec1[1]}   ATE: {round(spec1[2],2)}   SE: {round(spec1[3],2)}\")\n",
    "    else:\n",
    "        print(f\"   n: {spec1[0]}   AD: {round(spec1[1],2)}   SE: {round(spec1[2],2)}\")\n",
    "\n",
    "def for_tex(spec1, est_type):\n",
    "    if est_type == \"ATE\":\n",
    "        print(f\" & {spec1[0]} & {spec1[1]}   & {round(spec1[2],2)}   & {round(spec1[3],2)}\")\n",
    "    else:\n",
    "        print(f\" & {spec1[0]} & {round(spec1[1],2)}   & {round(spec1[2],2)}\")\n",
    "\n",
    "#########################################\n",
    "# Data Import & Processing (get_data)   #\n",
    "#########################################\n",
    "\n",
    "def get_data(df, spec, quintile, est_type=\"ATE\"):\n",
    "    if est_type == \"ATE\":\n",
    "        Y = df[\"net_tfa\"].values\n",
    "        D = df[\"e401\"].values\n",
    "        # Low–dim specification\n",
    "        X_L = df[[\"age\", \"inc\", \"educ\", \"fsize\", \"marr\", \"twoearn\", \"db\", \"pira\", \"hown\"]].values\n",
    "        # High–dim: use polynomial expansions for some variables\n",
    "        poly_age = PolynomialFeatures(degree=6, include_bias=False)\n",
    "        poly_inc = PolynomialFeatures(degree=8, include_bias=False)\n",
    "        poly_educ = PolynomialFeatures(degree=4, include_bias=False)\n",
    "        poly_fsize = PolynomialFeatures(degree=2, include_bias=False)\n",
    "        age_poly = poly_age.fit_transform(df[[\"age\"]].values)\n",
    "        inc_poly = poly_inc.fit_transform(df[[\"inc\"]].values)\n",
    "        educ_poly = poly_educ.fit_transform(df[[\"educ\"]].values)\n",
    "        fsize_poly = poly_fsize.fit_transform(df[[\"fsize\"]].values)\n",
    "        X_H = np.hstack([age_poly, inc_poly, educ_poly, fsize_poly, df[[\"marr\", \"twoearn\", \"db\", \"pira\", \"hown\"]].values])\n",
    "        # Very high–dim: using patsy (remove intercept later)\n",
    "        formula = \"poly(age,6,raw=True) + poly(inc,8,raw=True) + poly(educ,4,raw=True) + poly(fsize,2,raw=True) + marr + twoearn + db + pira + hown\"\n",
    "        X_vH = dmatrix(formula, df, return_type='dataframe').values[:, 1:]\n",
    "        if spec == 1:\n",
    "            X = X_L\n",
    "        elif spec == 2:\n",
    "            X = X_H\n",
    "        else:\n",
    "            X = X_vH\n",
    "        X = scale(X)\n",
    "        n = X.shape[0]\n",
    "        # Impose common support via a logistic model (no intercept)\n",
    "        clf = LogisticRegression(fit_intercept=False, multi_class='multinomial', solver='lbfgs')\n",
    "        clf.fit(X, D)\n",
    "        p1 = clf.predict_proba(X)[:, 1]\n",
    "        mask_D1 = (D == 1)\n",
    "        if np.sum(mask_D1) == 0:\n",
    "            min_p1, max_p1 = 0, 1\n",
    "        else:\n",
    "            min_p1 = p1[mask_D1].min()\n",
    "            max_p1 = p1[mask_D1].max()\n",
    "        indexes_to_drop = np.where((p1 < min_p1) | (p1 > max_p1))[0]\n",
    "        if len(indexes_to_drop) == 0:\n",
    "            indexes_to_drop = np.array([n])  # no dropping\n",
    "        mask = np.ones(n, dtype=bool)\n",
    "        mask[indexes_to_drop] = False\n",
    "        Y_trimmed = Y[mask]\n",
    "        D_trimmed = D[mask]\n",
    "        X_trimmed = X[mask, :]\n",
    "        if spec == 1:\n",
    "            inc = X_trimmed[:, 1]\n",
    "        else:\n",
    "            inc = X_trimmed[:, 7] if X_trimmed.shape[1] > 7 else X_trimmed[:, 1]\n",
    "        if quintile > 0:\n",
    "            q = pd.qcut(inc, 5, labels=False) + 1  # quintiles 1,...,5\n",
    "            mask_q = (q == quintile)\n",
    "            Y_q = Y_trimmed[mask_q]\n",
    "            D_q = D_trimmed[mask_q]\n",
    "            X_q = X_trimmed[mask_q, :]\n",
    "        else:\n",
    "            Y_q, D_q, X_q = Y_trimmed, D_trimmed, X_trimmed\n",
    "        return Y_q, D_q, X_q\n",
    "    else:\n",
    "        # PD type\n",
    "        N = df.shape[0]\n",
    "        cols_to_log = [\"gas\", \"price\", \"income\", \"age\", \"distance\"]\n",
    "        for col in cols_to_log:\n",
    "            df[col] = np.log(df[col])\n",
    "        Y = df[\"gas\"].values\n",
    "        df[\"age2\"] = df[\"age\"] ** 2\n",
    "        df[\"income2\"] = df[\"income\"] ** 2\n",
    "        df_up = df.copy()\n",
    "        df_down = df.copy()\n",
    "        prices = df[\"price\"].values\n",
    "        delta = np.std(prices) / 4\n",
    "        df_up[\"price\"] = prices + delta / 2\n",
    "        df_down[\"price\"] = prices - delta / 2\n",
    "        df[\"price2\"] = prices ** 2\n",
    "        df_up[\"price2\"] = df_up[\"price\"] ** 2\n",
    "        df_down[\"price2\"] = df_down[\"price\"] ** 2\n",
    "        # A simplified specification using patsy:\n",
    "        formula = \"price + price2 + urban + youngsingle + distance + I(distance**2) + age + age2 + income + income2\"\n",
    "        regressors = dmatrix(formula, df, return_type='dataframe').values\n",
    "        regressors_up = dmatrix(formula, df_up, return_type='dataframe').values\n",
    "        regressors_down = dmatrix(formula, df_down, return_type='dataframe').values\n",
    "        if quintile > 0:\n",
    "            q = pd.qcut(df[\"income\"], 5, labels=False) + 1\n",
    "            mask_q = (q == quintile)\n",
    "            Y_q = Y[mask_q]\n",
    "            regressors_q = regressors[mask_q, :]\n",
    "            regressors_up_q = regressors_up[mask_q, :]\n",
    "            regressors_down_q = regressors_down[mask_q, :]\n",
    "        else:\n",
    "            Y_q, regressors_q = Y, regressors\n",
    "            regressors_up_q, regressors_down_q = regressors_up, regressors_down\n",
    "        return Y_q, regressors_q, regressors_up_q, regressors_down_q, delta\n",
    "\n",
    "#################################\n",
    "# Main Execution of the Program #\n",
    "#################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "839c1a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ar8787/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_polynomial.py:555: RuntimeWarning: overflow encountered in multiply\n",
      "  np.multiply(\n",
      "/var/folders/r4/lwrzx5hs7d3cxk4p5zsy7zqh0000gp/T/ipykernel_92557/1517224467.py:9: RuntimeWarning: overflow encountered in power\n",
      "  return np.column_stack([x**i for i in range(1, degree+1)])\n",
      "/Users/ar8787/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:261: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\n",
      "/Users/ar8787/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:280: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\n",
      "/Users/ar8787/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_stata(\"/Users/ar8787/Dropbox/data_eco/Udesa/mater_thesis/raw/18515_Data_and_Programs/tutorial/sipp1991.dta\")\n",
    "spec = 1\n",
    "quintile = 0\n",
    "data = get_data(df, spec, quintile, est_type=\"ATE\")\n",
    "Y = data[0]\n",
    "T_var = data[1]\n",
    "X = data[2]\n",
    "dict_func = b\n",
    "p = len(b(T_var[0], X[0, :]))\n",
    "args_main = {'T': T_var}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "169a9cf0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 7\n",
      "k: 11\n",
      "fold: 1\n",
      "k: 7\n",
      "k: 11\n",
      "fold: 2\n",
      "k: 7\n",
      "k: 10\n",
      "fold: 3\n",
      "k: 7\n",
      "k: 11\n",
      "fold: 4\n"
     ]
    }
   ],
   "source": [
    "p0 = int(np.ceil(p / 4))\n",
    "if p > 60:\n",
    "    p0 = int(np.ceil(p / 40))\n",
    "D_LB = 0\n",
    "D_add = 0.2\n",
    "max_iter = 10\n",
    "np.random.seed(1)\n",
    "est_type = \"ATE\"\n",
    "alpha_estimator = 1  # 0: dantzig, 1: lasso\n",
    "gamma_estimator = 1  # 0: dantzig, 1: lasso, 2: rf, 3: nn, 4: 2-layer nn (works for ATE)\n",
    "bias = 0  # 0: unbiased, 1: biased\n",
    "results = rrr(args_main, est_type, Y, X, p0, D_LB, D_add, max_iter,\n",
    "              dict_func, alpha_estimator, gamma_estimator, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d1ad0a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3682, 6166, 5549.890651442612, 1497.4701731744835)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ea14c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_info = pd.read_csv('/Users/ar8787/Dropbox/data_eco/Udesa/mater_thesis/data/folds_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bec5e4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6736., 5918., 1012., ..., 4138., 8829., 5305.])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds_info.iloc[:, 4].dropna().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad38915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9f0c7aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "index = folds_info.iloc[:, i].dropna().astype(int).tolist()\n",
    "total_indices = np.arange( 0, 9848 )\n",
    "test_index = np.delete( total_indices, index, axis = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "878290f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfaux = pd.DataFrame({'index': np.arange( 0, 9848 )})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d28ac9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a64bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "# Choose type: \"ATE\" for average treatment effect or \"PD\" for elasticity\n",
    "type_ = \"PD\"  \n",
    "for quintile in [0]:\n",
    "    print(\"quintile:\", quintile)\n",
    "    if type_ == \"ATE\":\n",
    "        df = pd.read_stata(\"sipp1991.dta\")\n",
    "        spec = 1\n",
    "        data = get_data(df, spec, quintile, type_=\"ATE\")\n",
    "        Y = data[0]\n",
    "        T_var = data[1]\n",
    "        X = data[2]\n",
    "        dict_func = b\n",
    "        p = len(b(T_var[0], X[0, :]))\n",
    "        args_main = {'T': T_var}\n",
    "    else:\n",
    "        df = pd.read_stata(\"gasoline_final_tf1.dta\")\n",
    "        spec = 1\n",
    "        data = get_data(df, spec, quintile, type_=\"PD\")\n",
    "        Y = data[0]\n",
    "        X = data[1]\n",
    "        X_up = data[2]\n",
    "        X_down = data[3]\n",
    "        delta = data[4]\n",
    "        p = X.shape[1]\n",
    "        dict_func = b_PD\n",
    "        args_main = {'X.up': X_up, 'X.down': X_down, 'delta': delta}\n",
    "    p0 = int(np.ceil(p / 4))\n",
    "    if p > 60:\n",
    "        p0 = int(np.ceil(p / 40))\n",
    "    D_LB = 0\n",
    "    D_add = 0.2\n",
    "    max_iter = 10\n",
    "    np.random.seed(1)\n",
    "    alpha_estimator = 1  # 0: dantzig, 1: lasso\n",
    "    gamma_estimator = 1  # 0: dantzig, 1: lasso, 2: rf, 3: nn, 4: 2-layer nn (works for ATE)\n",
    "    bias = 0  # 0: unbiased, 1: biased\n",
    "    results = rrr(args_main, type_, Y, X, p0, D_LB, D_add, max_iter,\n",
    "                  dict_func, alpha_estimator, gamma_estimator, bias)\n",
    "    printer(results, type_)\n",
    "    for_tex(results, type_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
